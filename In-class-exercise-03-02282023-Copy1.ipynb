{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The third In-class-exercise (2/28/2023, 40 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to understand text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlease write you answer here:\\n1.\\nText classification or text mining task: Classifying text as either Positive or Negative sentiment.\\n\\nFeatures: \\n1) Word Count: Word count of the text can be used to identify the sentiment of the text. A higher word count will generally indicate a more positive sentiment, while a lower word count may indicate a more negative sentiment. \\n2) Sentiment Lexicon: Identifying words in the text that are associated with either positive or negative sentiment, such as happy, sad, good, bad, etc. \\n3) Content Analysis: Analyzing the context of the text, such as the overall topic, writing style and use of language, to identify the sentiment of the text.\\n4) Emotion Detection: Identifying emotions such as joy, anger, surprise, and so on, from the text by analyzing the facial expressions and body language used in the text.\\n5) Readability Analysis: Analyzing the readability of the text to determine the complexity of the language used, which can be used to identify the sentiment of the text.\\n6) Grammar Analysis: Identifying patterns of grammar and syntax in the text in order to identify the sentiment of the text.\\n\\nThese features could be helpful in building a machine learning model because they each have the potential to accurately identify the sentiment of the text. \\nWord count and sentiment lexicon capture the explicit sentiment of the text, while content analysis and emotion detection\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
    "\n",
    "'''\n",
    "Please write you answer here:\n",
    "1.\n",
    "Text classification or text mining task: Classifying text as either Positive or Negative sentiment.\n",
    "\n",
    "Features: \n",
    "1) Word Count: Word count of the text can be used to identify the sentiment of the text. A higher word count will generally indicate a more positive sentiment, while a lower word count may indicate a more negative sentiment. \n",
    "2) Sentiment Lexicon: Identifying words in the text that are associated with either positive or negative sentiment, such as happy, sad, good, bad, etc. \n",
    "3) Content Analysis: Analyzing the context of the text, such as the overall topic, writing style and use of language, to identify the sentiment of the text.\n",
    "4) Emotion Detection: Identifying emotions such as joy, anger, surprise, and so on, from the text by analyzing the facial expressions and body language used in the text.\n",
    "5) Readability Analysis: Analyzing the readability of the text to determine the complexity of the language used, which can be used to identify the sentiment of the text.\n",
    "6) Grammar Analysis: Identifying patterns of grammar and syntax in the text in order to identify the sentiment of the text.\n",
    "\n",
    "These features could be helpful in building a machine learning model because they each have the potential to accurately identify the sentiment of the text. \n",
    "Word count and sentiment lexicon capture the explicit sentiment of the text, while content analysis and emotion detection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Supraja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'barks', 'at', 'the', 'fox', 'but', 'the', 'fox', 'just', 'runs', 'away', '.']\n",
      "Sentences: ['\\nThe quick brown fox jumps over the lazy dog.', 'The dog barks at the fox but the fox just runs away.']\n",
      "Filtered words: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'dog', 'barks', 'fox', 'fox', 'runs', 'away']\n",
      "Word counts: Counter({'fox': 3, 'dog': 2, 'quick': 1, 'brown': 1, 'jumps': 1, 'lazy': 1, 'barks': 1, 'runs': 1, 'away': 1})\n",
      "Sentiment polarity: 0.04166666666666666\n",
      "Sentiment subjectivity: 0.75\n",
      "Bag-of-words: [[0 0 0 1 0 1 1 1 0 1 1 1 0 2]\n",
      " [1 1 1 0 1 1 2 0 1 0 0 0 1 3]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define a sample text data\n",
    "text_data = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. The dog barks at the fox but the fox just runs away.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text data into words and sentences\n",
    "words = word_tokenize(text_data)\n",
    "sentences = sent_tokenize(text_data)\n",
    "\n",
    "# Remove stop words and punctuation from the words list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word not in punctuation]\n",
    "\n",
    "# Count the frequency of each word in the filtered_words list\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "# Calculate the sentiment polarity and subjectivity of the text data using TextBlob\n",
    "blob = TextBlob(text_data)\n",
    "polarity = blob.sentiment.polarity\n",
    "subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "# Generate a bag-of-words representation of the text data using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Print the extracted text features\n",
    "print('Words:', words)\n",
    "print('Sentences:', sentences)\n",
    "print('Filtered words:', filtered_words)\n",
    "print('Word counts:', word_counts)\n",
    "print('Sentiment polarity:', polarity)\n",
    "print('Sentiment subjectivity:', subjectivity)\n",
    "print('Bag-of-words:', bag_of_words.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "1. Information Gain and Conditional Mutual Information: This is a measure of the relevance of a set of features to the target attribute. It identifies the most important features by considering both the relevance and dependence between features.\n",
    "2. Term Frequency-Inverse Document Frequency: This measure considers the frequency of a feature in a given sample and assigns more importance to those features that are more distinct and rare among all samples.\n",
    "3. Chi-Square Test: This is an independence measure that quantifies the strength of association between a set of features and the target attribute.\n",
    "4. Correlation-based Feature Selection: This method measures the correlation between features and the target attribute and selects those that are most strongly correlated. \n",
    "5. Genetic Algorithm: This method applies evolutionary algorithms to the feature selection problem by using a fitness value to evaluate the importance of each feature.\n",
    "6. Wrapper Method: This method evaluates the performance of a subset of features on some learning algorithm and selects those that yield the best performance.\n",
    "\n",
    "Ranking of Features Based on Their Importance:\n",
    "1. Information Gain and Conditional Mutual Information \n",
    "2. Term Frequency-Inverse Document Frequency \n",
    "3. Chi-Square Test \n",
    "4. Correlation-based Feature Selection\n",
    "5. ReliefF\n",
    "6. Genetic Algorithm\n",
    "7. Wrapper Method\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 15.0 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.0-py3-none-any.whl (199 kB)\n",
      "     ------------------------------------- 199.1/199.1 kB 11.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\supraja\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 23.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\supraja\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\supraja\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\supraja\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.1.1)\n",
      "Installing collected packages: tokenizers, pyyaml, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.0 pyyaml-6.0 tokenizers-0.13.2 transformers-4.26.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp310-cp310-win_amd64.whl (162.6 MB)\n",
      "     -------------------------------------- 162.6/162.6 MB 8.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.4.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in c:\\users\\supraja\\anaconda3\\envs\\myenv\\lib\\site-packages (1.13.1)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torchvision-0.14.1%2Bcu116-cp310-cp310-win_amd64.whl (4.8 MB)\n",
      "     ---------------------------------------- 4.8/4.8 MB 9.3 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torchaudio-0.13.1%2Bcu116-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "     ---------------------------------------- 2.3/2.3 MB 20.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\supraja\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\supraja\\appdata\\roaming\\python\\python310\\site-packages (from requests->torchvision) (1.26.13)\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-0.13.1+cu116 torchvision-0.14.1+cu116\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bfa5e51dcb4861b625410d5a5283c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Supraja\\anaconda3\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Supraja\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A brown dog and a white dog are playing in the park', 'The quick brown fox jumps over the lazy dog', 'The lazy dog is quick to bark but slow to bite']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Define the text data to be ranked\n",
    "text_data = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"The lazy dog is quick to bark but slow to bite\",\n",
    "    \"A brown dog and a white dog are playing in the park\"\n",
    "]\n",
    "\n",
    "# Define the query for matching relevant documents\n",
    "query = \"Brown dog playing in the park\"\n",
    "\n",
    "# Encode the query and text data using the BERT tokenizer\n",
    "encoded_query = tokenizer.encode(query, add_special_tokens=True, return_tensors='pt')\n",
    "encoded_text_data = [tokenizer.encode(text, add_special_tokens=True, return_tensors='pt') for text in text_data]\n",
    "\n",
    "# Use the BERT model to obtain the embeddings for the query and text data\n",
    "with torch.no_grad():\n",
    "    query_embedding = model(encoded_query)[0][:, 0, :]\n",
    "    text_data_embeddings = [model(encoded_text)[0][:, 0, :] for encoded_text in encoded_text_data]\n",
    "\n",
    "# Calculate the cosine similarity between the query and text data embeddings\n",
    "similarity_scores = [cosine_similarity(query_embedding, text_embedding)[0][0] for text_embedding in text_data_embeddings]\n",
    "\n",
    "# Rank the text data based on the similarity scores in descending order\n",
    "ranked_text_data = [text_data[i] for i in sorted(range(len(similarity_scores)), key=lambda k: similarity_scores[k], reverse=True)]\n",
    "\n",
    "# Print the ranked text data\n",
    "print(ranked_text_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
