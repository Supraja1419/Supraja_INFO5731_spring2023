{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The fifth in-class-exercise (40 points in total, 4/18/2023)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
    "\n",
    "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
    "\n",
    "Algorithms:\n",
    "\n",
    "(1) MultinominalNB\n",
    "\n",
    "(2) SVM \n",
    "\n",
    "(3) KNN \n",
    "\n",
    "(4) Decision tree\n",
    "\n",
    "(5) Random Forest\n",
    "\n",
    "(6) XGBoost\n",
    "\n",
    "(7) Word2Vec\n",
    "\n",
    "(8) BERT\n",
    "\n",
    "Evaluation measurement:\n",
    "\n",
    "(1) Accuracy\n",
    "\n",
    "(2) Recall\n",
    "\n",
    "(3) Precison \n",
    "\n",
    "(4) F-1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>apparently reassembled from the cutting-room f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>1</td>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>0</td>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0</td>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>0</td>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>1</td>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "0             1  a stirring , funny and finally transporting re...\n",
       "1             0  apparently reassembled from the cutting-room f...\n",
       "2             0  they presume their audience wo n't sit still f...\n",
       "3             1  this is a visually stunning rumination on love...\n",
       "4             1  jonathan parker 's bartleby should have been t...\n",
       "...         ...                                                ...\n",
       "6915          1  painful , horrifying and oppressively tragic ,...\n",
       "6916          0  take care is nicely performed by a quintet of ...\n",
       "6917          0  the script covers huge , heavy topics in a bla...\n",
       "6918          0  a seriously bad film with seriously warped log...\n",
       "6919          1  a deliciously nonsensical comedy about a city ...\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write your code here.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#Load and format the training data into a data frame\n",
    "sentiments = []\n",
    "texts = []\n",
    "\n",
    "with open('stsa-train.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        sentiment = int(parts[0])\n",
    "        text = parts[1]\n",
    "        sentiments.append(sentiment)\n",
    "        texts.append(text)\n",
    "\n",
    "train_data = pd.DataFrame({'sentiment': sentiments, 'text': texts})\n",
    "\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>no movement , no yuks , not much of anything .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>we never really feel involved with the story ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>this is one of polanski 's best films .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>0</td>\n",
       "      <td>an often-deadly boring , strange reading of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>0</td>\n",
       "      <td>the problem with concept films is that if the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>0</td>\n",
       "      <td>safe conduct , however ambitious and well-inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>0</td>\n",
       "      <td>a film made with as little wit , interest , an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>0</td>\n",
       "      <td>but here 's the real damn : it is n't funny , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "0             0     no movement , no yuks , not much of anything .\n",
       "1             0  a gob of drivel so sickly sweet , even the eag...\n",
       "2             0  gangs of new york is an unapologetic mess , wh...\n",
       "3             0  we never really feel involved with the story ,...\n",
       "4             1            this is one of polanski 's best films .\n",
       "...         ...                                                ...\n",
       "1816          0  an often-deadly boring , strange reading of a ...\n",
       "1817          0  the problem with concept films is that if the ...\n",
       "1818          0  safe conduct , however ambitious and well-inte...\n",
       "1819          0  a film made with as little wit , interest , an...\n",
       "1820          0  but here 's the real damn : it is n't funny , ...\n",
       "\n",
       "[1821 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = []\n",
    "texts = []\n",
    "\n",
    "with open('stsa-test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        sentiment = int(parts[0])\n",
    "        text = parts[1]\n",
    "        sentiments.append(sentiment)\n",
    "        texts.append(text)\n",
    "\n",
    "test_data = pd.DataFrame({'sentiment': sentiments, 'text': texts})\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Supraja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Supraja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('{html}',\"\")\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url = re.sub(r'http\\S+', '', cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)\n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words = [stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "# Apply the preprocessing to the training data\n",
    "train_data['text'] = train_data['text'].apply(preprocess)\n",
    "\n",
    "# Apply the preprocessing to the test data\n",
    "test_data['text'] = test_data['text'].apply(preprocess)\n",
    "\n",
    "# Convert the text data into a numerical representation using Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "y_train = train_data['sentiment']\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "y_test = test_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       stir funni final transport imagin beauti beast...\n",
       "1        appar reassembl cut room floor given daytim soap\n",
       "2       presum audienc sit still sociolog lesson howev...\n",
       "3       visual stun rumin love memori histori war art ...\n",
       "4       jonathan parker bartlebi end modern offic anom...\n",
       "                              ...                        \n",
       "6915                pain horrifi oppress tragic film miss\n",
       "6916    take care nice perform quintet actress nonethe...\n",
       "6917    script cover huge heavi topic bland surfacey w...\n",
       "6918    serious bad film serious warp logic writer dir...\n",
       "6919           delici nonsens comedi citi come apart seam\n",
       "Name: text, Length: 6920, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 movement yuk much anyth\n",
       "1       gob drivel sickli sweet even eager consum moor...\n",
       "2       gang new york unapologet mess whose save grace...\n",
       "3       never realli feel involv stori idea remain abs...\n",
       "4                                  one polanski best film\n",
       "                              ...                        \n",
       "1816    often deadli bore strang read classic whose wi...\n",
       "1817      problem concept film concept poor one save movi\n",
       "1818    safe conduct howev ambiti well intent fail hit...\n",
       "1819    film made littl wit interest profession artist...\n",
       "1820                               real damn funni either\n",
       "Name: text, Length: 1821, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data into training and validation data with a ratio of 80:20\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Mean accuracy:  0.7781823463745504\n",
      "Multinomial Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.74      0.77       671\n",
      "           1       0.77      0.84      0.81       713\n",
      "\n",
      "    accuracy                           0.79      1384\n",
      "   macro avg       0.79      0.79      0.79      1384\n",
      "weighted avg       0.79      0.79      0.79      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the Multinomial Naive Bayes model and perform 10-fold cross-validation on the training data\n",
    "nb = MultinomialNB()\n",
    "scores = cross_val_score(nb, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"Multinomial Naive Bayes Mean accuracy: \", scores.mean())\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_val)\n",
    "print(\"Multinomial Naive Bayes Classification Report:\\n\", classification_report(y_val, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Mean accuracy:  0.7577623856744635\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.73      0.77       671\n",
      "           1       0.77      0.84      0.80       713\n",
      "\n",
      "    accuracy                           0.79      1384\n",
      "   macro avg       0.79      0.78      0.78      1384\n",
      "weighted avg       0.79      0.79      0.78      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the SVM model and perform 10-fold cross-validation on the training data\n",
    "svm = SVC()\n",
    "scores = cross_val_score(svm, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"SVM Mean accuracy: \", scores.mean())\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "print(\"SVM Classification Report:\\n\", classification_report(y_val, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Mean accuracy:  0.5867069022920598\n",
      "KNN Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.61      0.57       671\n",
      "           1       0.58      0.51      0.54       713\n",
      "\n",
      "    accuracy                           0.56      1384\n",
      "   macro avg       0.56      0.56      0.56      1384\n",
      "weighted avg       0.56      0.56      0.56      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the KNN model and perform 10-fold cross-validation on the training data\n",
    "knn = KNeighborsClassifier()\n",
    "scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"KNN Mean accuracy: \", scores.mean())\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_val)\n",
    "print(\"KNN Classification Report:\\n\", classification_report(y_val, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean accuracy:  0.6585891853428298\n",
      "Classification Report for Decision Tree Model: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.67       671\n",
      "           1       0.69      0.71      0.70       713\n",
      "\n",
      "    accuracy                           0.69      1384\n",
      "   macro avg       0.69      0.69      0.69      1384\n",
      "weighted avg       0.69      0.69      0.69      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the Decision Tree model and perform 10-fold cross-validation on the training data\n",
    "dt = DecisionTreeClassifier()\n",
    "scores = cross_val_score(dt, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"Decision Tree Mean accuracy: \", scores.mean())\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_val)\n",
    "print(\"Classification Report for Decision Tree Model: \\n\", classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Mean accuracy:  0.7268744165399103\n",
      "Classification Report for Decision Tree Model: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.71       671\n",
      "           1       0.72      0.81      0.76       713\n",
      "\n",
      "    accuracy                           0.74      1384\n",
      "   macro avg       0.74      0.74      0.74      1384\n",
      "weighted avg       0.74      0.74      0.74      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the Random Forest model and perform 10-fold cross-validation on the training data\n",
    "rf = RandomForestClassifier()\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"Random Forest Mean accuracy: \", scores.mean())\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "print(\"Classification Report for Decision Tree Model: \\n\", classification_report(y_val, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Mean accuracy:  0.719656484812085\n",
      "Classification Report for Decision Tree Model: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73       671\n",
      "           1       0.74      0.74      0.74       713\n",
      "\n",
      "    accuracy                           0.73      1384\n",
      "   macro avg       0.73      0.73      0.73      1384\n",
      "weighted avg       0.73      0.73      0.73      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the XGBoost model and perform 10-fold cross-validation on the training data\n",
    "xgb = XGBClassifier()\n",
    "scores = cross_val_score(xgb, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"XGBoost Mean accuracy: \", scores.mean())\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_val)\n",
    "print(\"Classification Report for Decision Tree Model: \\n\", classification_report(y_val, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Supraja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=6899, vector_size=300, alpha=0.025>\n",
      "[-2.7902445e-04 -3.2790855e-03 -1.0501762e-03  4.1931868e-05\n",
      "  1.8708174e-03 -2.8985785e-03  1.2176129e-03 -3.1945610e-03\n",
      "  1.6634866e-03  2.5917320e-03  1.8078133e-03  2.8512545e-03\n",
      " -3.2797336e-04 -2.6543073e-03  1.3295221e-03  1.1467402e-03\n",
      "  2.2770402e-03  2.2057907e-03 -1.3995854e-03  1.6731803e-03\n",
      "  2.3202591e-03  1.2234731e-03 -3.3294440e-03 -3.0530309e-03\n",
      " -2.4362532e-03  1.4558275e-03  2.7001460e-04 -2.3820950e-03\n",
      " -8.9830277e-04  2.3827453e-03  1.3769603e-03 -5.2890100e-04\n",
      " -1.7734810e-03 -3.1183537e-03 -2.6763459e-03  2.2984720e-03\n",
      " -1.7644898e-03 -5.0507742e-04 -1.1627702e-03 -2.7044734e-03\n",
      "  2.2734403e-04 -1.1609737e-03 -6.9915928e-04  2.2314782e-03\n",
      " -2.0104253e-03 -1.7740738e-03  2.6385942e-03  1.4671132e-03\n",
      "  1.4610529e-04  9.0708176e-04 -9.7704690e-04  3.5650333e-04\n",
      " -9.0388855e-04 -1.8372322e-03  2.3970092e-03 -3.0308127e-04\n",
      "  2.1171998e-03  1.6272315e-03  1.3714135e-03 -1.5134811e-05\n",
      " -4.1125735e-04 -5.2097639e-05  1.3229628e-03  6.8773073e-04\n",
      " -1.5043180e-03 -3.1532010e-04  1.3954870e-03 -3.2466790e-03\n",
      "  1.6758248e-03  3.1526245e-03  2.2822733e-03  1.3156637e-03\n",
      "  2.9952248e-04 -2.2840006e-03 -2.7045112e-03  7.6517783e-04\n",
      "  2.1942905e-03 -2.2799810e-04 -2.6317548e-03  1.3734038e-03\n",
      " -2.5236162e-03  1.1042873e-03 -3.3326766e-03  2.3877688e-03\n",
      "  1.1372220e-03 -1.3183236e-04 -2.6491845e-03  1.8841847e-03\n",
      " -3.2467335e-03  1.7080426e-03 -2.2578503e-03 -6.0549023e-04\n",
      " -3.2806778e-03  7.4500640e-05  2.7494053e-03 -4.3654084e-04\n",
      "  1.0454981e-03 -2.8487567e-03 -3.8843034e-04  2.4143492e-03\n",
      "  3.0562123e-03  8.4037543e-04 -1.4193555e-03 -1.8862165e-03\n",
      " -2.6064774e-03  2.9092669e-03 -2.9025432e-03  1.6346737e-03\n",
      " -5.2218040e-04 -1.3897113e-03 -2.6229029e-03 -2.9252688e-04\n",
      " -2.4420761e-03  4.8571627e-04  2.0364774e-03  2.0382262e-03\n",
      "  3.0429976e-03  2.4227723e-03  4.0067156e-04  1.4277665e-03\n",
      "  4.7521153e-04 -1.9425626e-03 -7.2103855e-04 -3.2460105e-03\n",
      " -9.1420492e-04  2.3699296e-03  1.3509106e-03  2.3913216e-03\n",
      "  9.8423089e-04 -1.6605429e-03 -2.7341279e-03  3.1881821e-03\n",
      " -3.0271052e-03 -2.1070889e-03 -1.8258421e-03  1.0666824e-03\n",
      " -1.5117419e-03  1.4236820e-03  1.1995431e-03 -5.3224165e-05\n",
      " -2.4750372e-03  3.1595500e-03 -3.0707955e-04  4.6402216e-04\n",
      "  2.1044950e-03 -2.9442823e-03 -2.8647932e-03  7.9467695e-04\n",
      " -4.7317028e-04 -3.0709291e-03 -3.6196431e-04 -1.9712588e-03\n",
      "  2.4267157e-04 -2.4747613e-03  2.5030696e-03 -2.4624428e-04\n",
      "  2.9043094e-03 -1.7828838e-03 -1.0375805e-03 -1.3879478e-03\n",
      " -2.1864700e-03 -3.0172451e-03  1.9185472e-03  3.3931492e-04\n",
      " -2.4464834e-03  1.8054354e-03  2.6249194e-03  1.1814166e-03\n",
      "  8.3475711e-04 -1.3727562e-03  2.5420014e-03  1.5876254e-03\n",
      "  2.0251751e-04  2.0113429e-03  3.0521508e-03 -1.5188520e-03\n",
      " -1.1156070e-03 -1.3604645e-03  2.2527694e-03  2.6355537e-03\n",
      "  2.0501567e-03  1.4985454e-03 -3.0215674e-03 -6.5853039e-04\n",
      " -2.5902728e-03  6.3209335e-04 -9.0337795e-04  2.4210601e-03\n",
      " -2.4441914e-03  2.9652074e-03 -3.8125357e-04  2.1680426e-03\n",
      " -3.3147002e-03 -2.2109647e-03 -2.0519285e-03  1.6049087e-03\n",
      "  2.9752636e-03  4.6950777e-04 -1.7328342e-04 -3.5659868e-05\n",
      " -1.5370531e-03 -1.2674014e-04  7.8456121e-04 -1.8367624e-03\n",
      "  4.5223712e-04  8.7602419e-04 -2.8254632e-03 -3.3103228e-03\n",
      "  2.5344079e-03 -2.2929295e-03 -3.2708419e-03  7.1082512e-05\n",
      "  6.5690954e-04  1.3795181e-03  2.8373519e-04  2.3125596e-03\n",
      "  4.8845448e-04  1.4212629e-03  3.1441541e-03 -1.3489397e-03\n",
      " -2.5878151e-04  1.3949084e-03  6.7676662e-04  1.7359173e-03\n",
      "  3.2675492e-03 -1.6874790e-03  2.9625010e-03  2.2705793e-03\n",
      " -8.2374888e-04  1.9812819e-03  9.5803617e-04 -3.0525064e-03\n",
      " -1.6308168e-03  1.2548841e-03 -4.1330140e-04  3.2670859e-03\n",
      " -9.8315242e-04 -1.5730715e-03  6.1610143e-04 -8.0309706e-05\n",
      "  8.3011191e-04 -2.5319613e-03 -1.0824283e-04 -2.2828029e-03\n",
      " -2.7245299e-03  2.5250423e-03 -2.3351867e-04  1.3887545e-03\n",
      "  3.1023587e-03  5.6868116e-04 -7.6344411e-05  2.0092721e-03\n",
      " -1.3056493e-03  6.6888292e-04 -2.3876568e-03 -6.1694463e-04\n",
      "  2.7127480e-03  2.3803632e-03  1.7656906e-03 -3.0586584e-03\n",
      " -5.6571880e-04 -2.2769149e-03  6.4906396e-04 -1.3715704e-03\n",
      " -6.3278712e-04  2.7214852e-03 -4.8763712e-04 -1.3392469e-03\n",
      " -2.2112271e-03  9.7331882e-04 -3.0576321e-03  8.3318312e-04\n",
      "  2.6341649e-03  5.8425663e-05 -1.3907735e-03  1.3222587e-03\n",
      "  9.6562703e-04 -3.0701363e-03 -1.5598941e-03  3.0469254e-03\n",
      " -3.2568749e-03 -2.4237446e-03  2.5328200e-03 -2.8523987e-03\n",
      "  1.0180811e-03  2.1288013e-03 -6.6625274e-04 -3.1161089e-03\n",
      "  1.9499823e-03  2.6531117e-03  9.0399344e-04 -2.0088232e-03\n",
      "  1.2064012e-03 -1.6196752e-03 -8.2080084e-04  2.7671298e-03\n",
      "  1.8186601e-03  1.9681975e-03  8.6588663e-04 -9.8266604e-04]\n",
      "Word2Vec<vocab=6899, vector_size=300, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "#Training a Word2Vec Model\n",
    "nltk.download('punkt')\n",
    "# Tokenize the reviews into sentences\n",
    "sentences = [nltk.sent_tokenize(review) for review in train_data['text']]\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1, vector_size=300)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.key_to_index)\n",
    "# access vector for one sentence\n",
    "print(model.wv.get_vector('offer new insight matter charact exactli spring life'))\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation score: 0.5233018455291452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       671\n",
      "           1       0.52      1.00      0.68       713\n",
      "\n",
      "    accuracy                           0.52      1384\n",
      "   macro avg       0.26      0.50      0.34      1384\n",
      "weighted avg       0.27      0.52      0.35      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define a function to vectorize a sentence\n",
    "def vectorize_sentence(sentence, model):\n",
    "    \"\"\"\n",
    "    Given a sentence and a Word2Vec model, return the sentence vector\n",
    "    by averaging the word vectors in the sentence.\n",
    "    \"\"\"\n",
    "    # Combine the words in the sentence into a single string\n",
    "    sentence = ' '.join(sentence)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    # Split sentence into words\n",
    "    words = sentence.split()\n",
    "    # Filter out words that are not in the Word2Vec model\n",
    "    words = [word for word in words if word in model.wv]\n",
    "    # If there are no words in the sentence that are in the Word2Vec model, return a zero vector\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    # Otherwise, return the average of the word vectors in the sentence\n",
    "    else:\n",
    "        return np.mean([model.wv[word] for word in words], axis=0)\n",
    "\n",
    "\n",
    "# Vectorize the train set\n",
    "X_train_vec = np.array([vectorize_sentence(sentence, model) for sentence in sentences])\n",
    "y_train = train_data['sentiment']\n",
    "\n",
    "# Split train set into train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_vec, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the test set\n",
    "X_test = np.array([vectorize_sentence(sentence, model) for sentence in test_data['text']])\n",
    "y_test = test_data['sentiment']\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train SVM model\n",
    "svm = SVC(kernel='linear', random_state=42,)\n",
    "# Perform 10-fold cross-validation on the training data\n",
    "scores = cross_val_score(svm, X_train, y_train, cv=10)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print('Mean cross-validation score:', scores.mean())\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = svm.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
    "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
    "(You can also use different text data which you want)\n",
    "\n",
    "Apply the listed clustering methods to the dataset:\n",
    "\n",
    "K-means\n",
    "\n",
    "DBSCAN\n",
    "\n",
    "Hierarchical clustering\n",
    "\n",
    "Word2Vec\n",
    "\n",
    "BERT\n",
    "\n",
    "You can refer to of the codes from  the follwing link below. \n",
    "https://www.kaggle.com/karthik3890/text-clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Supraja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      I feel so LUCKY to have found this used (phone...\n",
       "1      nice phone, nice up grade from my pantach revu...\n",
       "2                                           Very pleased\n",
       "3      It works good but it goes slow sometimes but i...\n",
       "4      Great phone to replace my lost phone. The only...\n",
       "                             ...                        \n",
       "995    It's a decent for the price.. I've had this on...\n",
       "996                                   Is good cell phone\n",
       "997    Amazing phone. Cables and case included, also ...\n",
       "998                                             Excelent\n",
       "999       Excellent, it meets the requirements requested\n",
       "Name: Reviews, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new =df.head(1000)\n",
    "new['Reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Lemmatization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Lemmatization'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m tfidf_vect \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Use the fit_transform method of the TfidfVectorizer object to generate a TF-IDF matrix from the 'Lemmatization' column of the 'new' dataframe\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m tfidf_vect\u001b[38;5;241m.\u001b[39mfit_transform(new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLemmatization\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Print the shape of the resulting TF-IDF matrix\u001b[39;00m\n\u001b[0;32m     11\u001b[0m tfidf\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Lemmatization'"
     ]
    }
   ],
   "source": [
    "#Implementing kmeans usind TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Import the TfidfVectorizer class for generating TF-IDF features\n",
    "\n",
    "# Instantiate a TfidfVectorizer object with default parameters\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# Use the fit_transform method of the TfidfVectorizer object to generate a TF-IDF matrix from the 'Lemmatization' column of the 'new' dataframe\n",
    "tfidf = tfidf_vect.fit_transform(new['Lemmatization'].values)\n",
    "\n",
    "# Print the shape of the resulting TF-IDF matrix\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model_tf \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m99\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Use the fit method of the KMeans object to fit the model to the TF-IDF matrix generated in the previous step\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model_tf\u001b[38;5;241m.\u001b[39mfit(tfidf)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans  # Import the KMeans class from scikit-learn\n",
    "\n",
    "# Instantiate a KMeans object with 10 clusters and a random state of 99\n",
    "model_tf = KMeans(n_clusters=10, random_state=99)\n",
    "\n",
    "# Use the fit method of the KMeans object to fit the model to the TF-IDF matrix generated in the previous step\n",
    "model_tf.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KMeans' object has no attribute 'labels_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assign the cluster labels generated by the KMeans algorithm to a variable\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m labels_tf \u001b[38;5;241m=\u001b[39m model_tf\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assign the cluster centers generated by the KMeans algorithm to a variable\u001b[39;00m\n\u001b[0;32m      5\u001b[0m cluster_center_tf \u001b[38;5;241m=\u001b[39m model_tf\u001b[38;5;241m.\u001b[39mcluster_centers_\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KMeans' object has no attribute 'labels_'"
     ]
    }
   ],
   "source": [
    "# Assign the cluster labels generated by the KMeans algorithm to a variable\n",
    "labels_tf = model_tf.labels_\n",
    "\n",
    "# Assign the cluster centers generated by the KMeans algorithm to a variable\n",
    "cluster_center_tf = model_tf.cluster_centers_\n",
    "\n",
    "# Print the cluster centers to the console\n",
    "print(cluster_center_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the get_feature_names_out method of the TfidfVectorizer object to extract the terms in the corpus\n",
    "terms1 = tfidf_vect.get_feature_names_out()\n",
    "\n",
    "# Print the first 100 terms to the console\n",
    "print(terms1[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = new  # Assign the DataFrame 'new' to a new variable 'df1'\n",
    "df1['Tfidf Clus Label'] = model_tf.labels_  # Add a new column 'Tfidf Clus Label' to 'df1' containing the clustering labels obtained from 'model_tf'\n",
    "df1[['Lemmatization','Tfidf Clus Label']].head()  # Select the 'Lemmatization' and 'Tfidf Clus Label' columns from 'df1' and display the first five rows using the `head()` method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby(['Tfidf Clus Label'])['Reviews'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top clusters:\")\n",
    "order_centroids = model_tf.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(10):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms1[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"4 reviews of ensured to cluster \", i)\n",
    "    print(\"-\" * 70)\n",
    "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][0]]['Reviews'])\n",
    "    print('\\n')\n",
    "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][2]]['Reviews'])\n",
    "    print('\\n')\n",
    "    print(df1.iloc[df1.groupby(['Tfidf Clus Label']).groups[i][6]]['Reviews'])\n",
    "    print('\\n')\n",
    "    print(\"_\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating bag of words features and Implementing K-means.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "bow = count_vect.fit_transform(new['Reviews'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans  # Importing the KMeans clustering algorithm from sklearn.cluster library\n",
    "model = KMeans(n_clusters = 10,init='k-means++',random_state=99)  # Creating an instance of the KMeans clustering algorithm with 10 clusters, k-means++ initialization, and a fixed random state of 99.\n",
    "model.fit(bow)  # Fitting the KMeans clustering model on the bag-of-words (bow) matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.labels_  # Obtaining the cluster labels for each data point in the input matrix using the 'labels_' attribute of the KMeans model\n",
    "cluster_center = model.cluster_centers_  # Obtaining the cluster centers (centroids) for each of the clusters using the 'cluster_centers_' attribute of the KMeans model\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.silhouette_score(bow, labels, metric='euclidean'))\n",
    "\n",
    "\n",
    "new['Bow Label'] = model.labels_ \n",
    "new[['Lemmatization','Bow Label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing DBSCAN\n",
    "from sklearn.cluster import DBSCAN  # Importing the DBSCAN clustering algorithm from the sklearn.cluster library\n",
    "import numpy as np  # Importing the numpy library and aliasing it as np\n",
    "\n",
    "minPts = 2 * 100  # Setting the value of the minimum points parameter for DBSCAN algorithm to be twice the dimensionality of the dataset\n",
    "\n",
    "def lower_bound(nums, target):  # Defining a function to return the number in the array just greater than or equal to itself\n",
    "    l, r = 0, len(nums) - 1\n",
    "    while l <= r:  # Implementing binary search to find the nearest number\n",
    "        mid = int(l + (r - l) / 2)\n",
    "        if nums[mid] >= target:\n",
    "            r = mid - 1\n",
    "        else:\n",
    "            l = mid + 1\n",
    "    return l\n",
    "\n",
    "def compute200thnearestneighbour(x, data):  # Defining a function to compute the 200th nearest neighbor of a point in the dataset\n",
    "    dists = []  # Initializing an empty list to store the distances\n",
    "    for val in data:  # Computing the distance between the given point and all other points in the dataset\n",
    "        dist = np.sum((x - val) **2 )\n",
    "        if (len(dists) == 200 and dists[199] > dist): \n",
    "          l = int(lower_bound(dists, dist)) \n",
    "          if l < 200 and l >= 0 and dists[l] > dist:\n",
    "              dists[l] = dist\n",
    "        else:\n",
    "          dists.append(dist)\n",
    "          dists.sort()\n",
    "    \n",
    "    return dists[199]  # Returning the distance to the 200th nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sent_train = list()\n",
    "\n",
    "for i in new[\"Lower casing\"].values:\n",
    "  list_of_sent_train.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Train Word2Vec model\n",
    "list_of_sent_train = list()\n",
    "for i in new[\"Lower casing\"].values:\n",
    "  list_of_sent_train.append(i.split())\n",
    "w2v_model = gensim.models.Word2Vec(list_of_sent_train, workers=4)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create sentence vectors using trained Word2Vec model\n",
    "sent_vectors = []\n",
    "count = 1\n",
    "for sent in list_of_sent_train:\n",
    "    sent_vec = np.zeros(100)\n",
    "    cnt_words = 1\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "sent_vectors = np.array(sent_vectors)\n",
    "sent_vectors = np.nan_to_num(sent_vectors)\n",
    "\n",
    "# Calculate 200th nearest neighbor for each sentence vector\n",
    "twohundrethneigh = []\n",
    "for val in sent_vectors[:300]:\n",
    "    twohundrethneigh.append(compute200thnearestneighbour(val, sent_vectors[:300]))\n",
    "twohundrethneigh.sort()\n",
    "\n",
    "# Train DBSCAN model\n",
    "model = DBSCAN(eps=5, min_samples=minPts, n_jobs=-1)\n",
    "model.fit(sent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new['AVG-W2V Clus Label'] = model.labels_\n",
    "new[['Lemmatization','AVG-W2V Clus Label']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Hierarchial Clustering\n",
    "import scipy\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Instantiate AgglomerativeClustering object with the number of clusters, affinity measure, and linkage criteria\n",
    "cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward') \n",
    "\n",
    "# Fit the clustering model to the sent_vectors data using the fit_predict() method\n",
    "Agg = cluster.fit_predict(sent_vectors)\n",
    "\n",
    "# Create a new DataFrame called \"hier\" with the same data as the \"new\" DataFrame\n",
    "hier = new\n",
    "\n",
    "# Assign the cluster labels to the AVG-W2V Clus Label column of the hier DataFrame\n",
    "hier['AVG-W2V Clus Label'] = cluster.labels_\n",
    "\n",
    "# Display the first 5 rows of the hier DataFrame\n",
    "hier.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier.groupby(['AVG-W2V Clus Label'])['Reviews'].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can write you answer here. (No code needed)\n",
    "Based on Euclidean distance, K-means allocates each data point to the closest cluster center.\n",
    "Given that the clusters are clearly delineated based on categories or products, K-means outperforms DBSCAN on TFIDF. \n",
    "While DBSCAN uses a density criterion to group together data points that are close to one another. \n",
    "However, all reviews are evenly grouped for average word vectors. \n",
    "DBSCAN performance on a dataset with is subpar since it clusters all reviews together.\n",
    "By repeatedly merging or dividing existing clusters according to their proximity, hierarchical clustering creates a hierarchy of clusters. \n",
    "Because the clusters are not evenly distributed and are challenging to identify, hierarchical clustering does not perform well.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
