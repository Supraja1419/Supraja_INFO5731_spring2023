<a href="https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Four.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# **INFO5731 Assignment Four**

In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**.

# **Question 1: Topic Modeling**

(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:

(1) Features (text representation) used for topic modeling.

(2) Top 10 clusters for topic modeling.

(3) Summarize and describe the topic for each cluster. 


# Write your code here
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from gensim import corpora
from gensim.models.ldamodel import LdaModel



df = pd.read_csv('https://raw.githubusercontent.com/jeevankumar20/INFO_5731_Spring2023/main/annotated_reviews.csv')
df


from gensim import corpora
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel


# Tokenize the text
corpus = df['Cleaned Reviews'].apply(str.split)

# Create a dictionary with the corpus
dictionary = corpora.Dictionary(corpus)

# Convert corpus into a bag of words
bow_corpus = [dictionary.doc2bow(text) for text in corpus]

# Build the LDA model
num_topics = 10
lda_model = LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=num_topics, passes=10, alpha='auto')

# Print the top 10 clusters/topics
for index, topic in enumerate(lda_model.show_topics(num_topics=num_topics)):
    print(f"Topic {index}:")
    print(topic)

corpus_lda = lda_model[bow_corpus]
num_topics = 10
scores = [[] for i in range(num_topics)]
for doc in corpus_lda:
    for i in range(num_topics):
        if len(doc) > i:
            scores[i].append(round(doc[i][1], 2))
        else:
            scores[i].append(0)

# Create data frame that shows scores assigned for each topic for each review
df_topic = pd.DataFrame()
df_topic['Text'] = df['Cleaned Reviews']
for i in range(num_topics):
    df_topic[f'Topic {i} score'] = scores[i]
df_topic['Topic'] = df_topic[[f'Topic {i} score' for i in range(num_topics)]].apply(lambda x: x.argmax(), axis=1)
df_topic.head(10)

# **Question 2: Sentiment Analysis**

(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  

(1) Features used for sentiment classification and explain why you select these features.

(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.

(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. 

# Write your code here
import pandas as pd
import warnings
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
warnings.filterwarnings("ignore")
# load data

X = df['Cleaned Reviews']
y = df['sentiment']

# split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# preprocess text data using TF-IDF vectorizer
tfidf = TfidfVectorizer(stop_words='english')
X_train = tfidf.fit_transform(X_train)
X_test = tfidf.transform(X_test)

# select features for classification
features_train = X_train
features_test = X_test

# train and evaluate Naive Bayes classifier
nb = MultinomialNB()
nb.fit(features_train, y_train)
nb_preds = nb.predict(features_test)
nb_report = classification_report(y_test, nb_preds)

# train and evaluate Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(features_train, y_train)
rf_preds = rf.predict(features_test)
rf_report = classification_report(y_test, rf_preds)

# cross-validation scores for Naive Bayes and Random Forest
nb_scores = cross_val_score(nb, features_train, y_train, cv=5)
rf_scores = cross_val_score(rf, features_train, y_train, cv=5)

# print performance metrics
print('Naive Bayes Classification Report:\n', nb_report)
print('Random Forest Classification Report:\n', rf_report)
print('Naive Bayes Cross-Validation Scores:', nb_scores)
print('Random Forest Cross-Validation Scores:', rf_scores)




# **Question 3: House price prediction**

(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. 



# Write your code here

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')



train = pd.read_csv('train.csv')

test = pd.read_csv('test.csv')



train


train.describe()

test

test.describe()

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd



# extract features and target variable
X = train.drop("SalePrice", axis=1)
y = train["SalePrice"]


# identify categorical features
cat_features = X.select_dtypes(include=['object']).columns.tolist()

# define column transformer to apply one-hot encoding
preprocessor = ColumnTransformer(
    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)],
    remainder='passthrough')

# split data into 80-20 train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# fill missing numerical values with mean
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)

# fill missing categorical values with mode
X_train.fillna(X_train.mode().iloc[0], inplace=True)
X_test.fillna(X_test.mode().iloc[0], inplace=True)

# fit a linear regression model on the train set
lm = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearRegression())])
lm.fit(X_train, y_train)

# make predictions on the test set
y_pred = lm.predict(X_test)
print("Predicted House Prices: ",y_pred)

from sklearn.metrics import mean_squared_error, r2_score
# compute the R-squared value
r2 = r2_score(y_test, y_pred)
r2
